"""
Labyrinthia AI - æ¸¸æˆé…ç½®å¯¼å…¥æ–‡ä»¶
Configuration file for the Labyrinthia AI game
"""

import os
import logging
from typing import Dict, Any, Optional
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from dotenv import load_dotenv

# é…ç½®åŸºç¡€æ—¥å¿—ï¼ˆåœ¨å¯¼å…¥å…¶ä»–æ¨¡å—ä¹‹å‰ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åŠ è½½ .env æ–‡ä»¶
_env_loaded = load_dotenv()


class LLMProvider(Enum):
    """LLMæœåŠ¡æä¾›å•†æšä¸¾"""
    GEMINI = "gemini"
    OPENAI = "openai"
    LMSTUDIO = "lmstudio"
    OPENROUTER = "openrouter"


@dataclass
class _GeminiConfig:
    """Gemini Provider Specific Config"""
    api_key: str = ""  # ä»ç¯å¢ƒå˜é‡åŠ è½½
    model_name: str = "gemini-2.0-flash"
    endpoint: str = "https://generativelanguage.googleapis.com"
    api_version: str = "v1beta"

@dataclass
class _OpenRouterConfig:
    """OpenRouter Provider Specific Config"""
    api_key: str = ""  # ä»ç¯å¢ƒå˜é‡åŠ è½½
    model_name: str = "google/gemini-2.0-flash-001"
    base_url: str = "https://openrouter.ai/api/v1"

@dataclass
class _OpenAIConfig:
    """OpenAI Provider Specific Config"""
    # OpenAIå…¼å®¹æœåŠ¡å™¨é…ç½®ï¼ˆå®é™…ä½¿ç”¨ä¸­è½¬åçš„ Gemini æ¨¡å‹ï¼‰
    api_key: str = ""  # ä»ç¯å¢ƒå˜é‡åŠ è½½
    model_name: str = "gemini-2.0-flash"
    base_url: str = "https://ai.yanshanlaosiji.top/v1"
    # å›¾ç‰‡ç”Ÿæˆæ¨¡å‹é…ç½®
    image_model: str = "imagen-4.0-ultra-generate-001"
    # TTSæ¨¡å‹é…ç½®
    tts_model: str = "tts-1"

@dataclass
class _LMStudioConfig:
    """LMStudio Provider Specific Config (Future Implementation)"""
    api_key: str = "lm-studio"
    model_name: str = "local-model"
    base_url: str = "http://localhost:1234/v1"


@dataclass
class LLMConfig:
    """LLMé…ç½®ç±»"""
    # ------------------- LLM Provider Master Configuration --------------------
    # åœ¨æ­¤å¤„é€‰æ‹©è¦ä½¿ç”¨çš„LLMä¾›åº”å•†
    # Load from `LLM_PROVIDER` environment variable first.
    provider: LLMProvider = LLMProvider.OPENAI

    # å„ä¸ªä¾›åº”å•†çš„è¯¦ç»†é…ç½® (å¯åœ¨æ­¤å¤„ä¿®æ”¹é»˜è®¤å€¼)
    gemini: _GeminiConfig = field(default_factory=_GeminiConfig)
    openrouter: _OpenRouterConfig = field(default_factory=_OpenRouterConfig)
    openai: _OpenAIConfig = field(default_factory=_OpenAIConfig)
    lmstudio: _LMStudioConfig = field(default_factory=_LMStudioConfig)
    # --------------------------------------------------------------------------

    # --- Active LLM Configuration (ç”±ä¸Šé¢é€‰æ‹©çš„provideråŠ¨æ€å¡«å……) ---
    # --- (DO NOT EDIT THESE FIELDS DIRECTLY) ---
    api_key: str = ""
    model_name: str = ""
    # é€šç”¨ç”Ÿæˆå‚æ•°ï¼ˆä»ç¯å¢ƒå˜é‡åŠ è½½ï¼Œè§ _load_from_envï¼‰
    max_output_tokens: Optional[int] = None  # ä¸è®¾ç½®é™åˆ¶ï¼Œé¿å…æ€è€ƒè¿‡ç¨‹è¢«æˆªæ–­
    use_generation_params: bool = False  # æ˜¯å¦ä½¿ç”¨temperatureå’Œtop_på‚æ•°ï¼ŒFalseæ—¶ä½¿ç”¨LLMé»˜è®¤å€¼
    temperature: float = 0.8
    top_p: float = 0.9
    timeout: int = 120
    # å†å²è®°å½•ç®¡ç†å‚æ•°
    max_history_tokens: int = 10240  # å†å²è®°å½•æœ€å¤§tokenæ•°é‡
    min_context_entries: int = 5  # æœ€å°ä¿ç•™çš„ä¸Šä¸‹æ–‡æ¡ç›®æ•°
    context_cleanup_threshold: float = 0.8  # ä¸Šä¸‹æ–‡æ¸…ç†è§¦å‘é˜ˆå€¼
    # ---- LLM ä¸Šä¸‹æ–‡è®°å½•å¼€å…³ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰ ----
    record_combat_to_context: bool = True   # æ˜¯å¦è®°å½•æˆ˜æ–—äº‹ä»¶åˆ°ä¸Šä¸‹æ–‡
    record_trap_to_context: bool = True     # æ˜¯å¦è®°å½•é™·é˜±äº‹ä»¶åˆ°ä¸Šä¸‹æ–‡
    # ---- LLM ä¸Šä¸‹æ–‡æ³¨å…¥æ§åˆ¶ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰ ----
    inject_context_to_prompt: bool = True      # æ˜¯å¦å°†æœ€è¿‘æ¸¸æˆä¸Šä¸‹æ–‡æ³¨å…¥åˆ°æç¤ºè¯
    context_max_entries: int = 12              # æ³¨å…¥çš„ä¸Šä¸‹æ–‡æœ€å¤§æ¡ç›®æ•°
    context_include_metadata: bool = False     # æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡å—ä¸­åŒ…å«å…ƒæ•°æ®

    # ---- LLM ä¸Šä¸‹æ–‡æŒä¹…åŒ–æ§åˆ¶ ----
    save_context_entries: int = 20             # å­˜æ¡£ä¸­ä¿å­˜çš„ä¸Šä¸‹æ–‡æ¡ç›®æ•°ä¸Šé™

    # åŠ¨æ€å¡«å……çš„ç‰¹å®šäºæä¾›å•†çš„URL
    gemini_endpoint: str = ""
    gemini_api_version: str = ""
    openrouter_base_url: str = ""
    openai_base_url: str = ""
    lmstudio_base_url: str = ""
    # --------------------------------------------------------------------------

    # -------------------------- Proxy Configuration ---------------------------
    use_proxy: bool = False
    proxy_url: str = "http://127.0.0.1:10808"
    proxy_username: str = ""
    proxy_password: str = ""
    # --------------------------------------------------------------------------

    # ------------------------ Encoding Configuration -------------------------
    # ç¼–ç è½¬æ¢é…ç½® - ç”¨äºè§£å†³UbuntuæœåŠ¡å™¨ä¸Šçš„å­—ç¬¦ç¼–ç é—®é¢˜
    use_encoding_conversion: bool = False            # æ˜¯å¦å¯ç”¨ç¼–ç è½¬æ¢ï¼ˆWindowsç¯å¢ƒé»˜è®¤å…³é—­ï¼‰
    encoding_method: str = "utf8_strict"             # ç¼–ç æ–¹æ³•ï¼š utf8_strict, json_escape
    force_utf8_encoding: bool = True                 # å¼ºåˆ¶UTF-8ç¼–ç 

    # ç¼–ç æ€§èƒ½é…ç½®
    encoding_timeout: float = 30.0                   # ç¼–ç æ“ä½œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    encoding_retry_count: int = 2                    # ç¼–ç å¤±è´¥é‡è¯•æ¬¡æ•°
    encoding_fallback_enabled: bool = True          # å¯ç”¨ç¼–ç å¤±è´¥å›é€€æœºåˆ¶
    show_encoding_impact: bool = True                # æ˜¾ç¤ºç¼–ç å¯¹å¤§å°çš„å½±å“




@dataclass
class DebugConfig:
    """è°ƒè¯•é…ç½®ç±»"""
    # åŸºç¡€è°ƒè¯•è®¾ç½®
    enabled: bool = True                             # å¯ç”¨è°ƒè¯•æ¨¡å¼
    show_llm_debug: bool = True                      # æ˜¾ç¤ºLLMè°ƒè¯•ä¿¡æ¯
    show_encoding_debug: bool = True                 # æ˜¾ç¤ºç¼–ç è°ƒè¯•ä¿¡æ¯
    show_performance_metrics: bool = True            # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡

    # ç¼–ç è°ƒè¯•è®¾ç½®
    log_encoding_operations: bool = True             # è®°å½•ç¼–ç æ“ä½œ
    show_size_comparisons: bool = True               # æ˜¾ç¤ºå¤§å°å¯¹æ¯”
    log_encoding_failures: bool = True               # è®°å½•ç¼–ç å¤±è´¥

    # æ€§èƒ½è°ƒè¯•è®¾ç½®
    measure_request_time: bool = True                # æµ‹é‡è¯·æ±‚æ—¶é—´
    measure_encoding_time: bool = True               # æµ‹é‡ç¼–ç æ—¶é—´
    log_slow_operations: bool = True                 # è®°å½•æ…¢æ“ä½œ
    slow_operation_threshold: float = 1.0            # æ…¢æ“ä½œé˜ˆå€¼ï¼ˆç§’ï¼‰

    # æ—¥å¿—è®¾ç½®
    log_level: str = "DEBUG"                         # æ—¥å¿—çº§åˆ«
    log_to_file: bool = True                         # è®°å½•åˆ°æ–‡ä»¶
    log_file_max_size: int = 10485760               # æ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°ï¼ˆ10MBï¼‰
    log_file_backup_count: int = 5                   # æ—¥å¿—æ–‡ä»¶å¤‡ä»½æ•°é‡


@dataclass
class GameConfig:
    """æ¸¸æˆé…ç½®ç±»"""
    # åŸºç¡€æ¸¸æˆè®¾ç½®
    game_name: str = "Labyrinthia AI"
    version: str = "1.0.0"
    debug_mode: bool = True
    show_llm_debug: bool = True  # æ˜¯å¦æ˜¾ç¤ºLLMè°ƒè¯•ä¿¡æ¯

    # åœ°å›¾è®¾ç½®
    default_map_size: tuple = (20, 20)
    max_map_size: tuple = (50, 50)
    min_map_size: tuple = (10, 10)

    # è§’è‰²è®¾ç½®
    max_player_level: int = 20
    starting_level: int = 1
    starting_hp: int = 100
    starting_mp: int = 50

    # æˆ˜æ–—è®¾ç½®
    max_combat_rounds: int = 50
    critical_hit_chance: float = 0.05

    # æˆ˜æ–—å™è¿°è®¾ç½®
    enable_combat_narrative: bool = True           # å¯ç”¨æˆ˜æ–—å™è¿°ç”Ÿæˆ
    boss_defeat_full_context: bool = True          # Bosså‡»è´¥ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡
    quest_monster_full_context: bool = True        # ä»»åŠ¡æ€ªç‰©å‡»è´¥ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡
    normal_monster_full_context: bool = False      # æ™®é€šæ€ªç‰©å‡»è´¥ä½¿ç”¨ç®€åŒ–ä¸Šä¸‹æ–‡

    # å†…å®¹ç”Ÿæˆè®¾ç½®
    enable_ai_generation: bool = True
    batch_generation_size: int = 5  # æ‰¹é‡ç”Ÿæˆå†…å®¹çš„æ•°é‡
    content_cache_size: int = 100   # å†…å®¹ç¼“å­˜å¤§å°

    # å­˜æ¡£è®¾ç½®ï¼ˆä»ç¯å¢ƒå˜é‡åŠ è½½ï¼Œè§ _load_from_envï¼‰
    max_save_slots: int = 10
    auto_save_interval: int = 300  # ç§’
    game_session_timeout: int = 3600  # æ¸¸æˆä¼šè¯è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œ1å°æ—¶æ— æ´»åŠ¨åè‡ªåŠ¨å…³é—­
    max_active_games_per_user: int = 5  # æ¯ä¸ªç”¨æˆ·æœ€å¤šåŒæ—¶æ´»è·ƒçš„æ¸¸æˆæ•°é‡

    # æ€§èƒ½è®¾ç½®ï¼ˆä»ç¯å¢ƒå˜é‡åŠ è½½ï¼Œè§ _load_from_envï¼‰
    max_concurrent_llm_requests: int = 3
    request_retry_count: int = 3
    request_retry_delay: float = 1.0

    # ä»»åŠ¡è¿›åº¦æ§åˆ¶è®¾ç½®ï¼ˆå·²ä¼˜åŒ–ï¼‰
    max_quest_floors: int = 3                   # å¼€å‘é˜¶æ®µï¼šä»»åŠ¡æœ€å¤§æ¥¼å±‚æ•°
    # æ³¨æ„ï¼šä»»åŠ¡è¿›åº¦åœ¨UIä¸­å§‹ç»ˆæ˜¾ç¤ºï¼Œä¸å—è°ƒè¯•æ¨¡å¼æ§åˆ¶

    # ã€é‡è¦ä¿®å¤ã€‘è¿›åº¦å¢é‡é…ç½® - åŸºäºæ¥¼å±‚å˜åŒ–è€Œéç»å¯¹æ·±åº¦
    map_transition_progress: float = 18.0       # åœ°å›¾åˆ‡æ¢è¿›åº¦å¢é‡ï¼ˆæ¯æ¬¡åˆ‡æ¢æ¥¼å±‚å¢åŠ 18%ï¼‰
    max_single_progress_increment: float = 25.0 # å•æ¬¡è¿›åº¦å¢é‡ä¸Šé™ï¼ˆé¿å…è·³è·ƒè¿‡å¤§ï¼‰

    # è¿›åº¦ç®¡ç†å™¨è®¾ç½®
    enable_smart_progression: bool = True       # å¯ç”¨æ™ºèƒ½è¿›åº¦æ¨è¿›
    progress_history_limit: int = 100          # è¿›åº¦å†å²è®°å½•é™åˆ¶
    auto_quest_completion: bool = True         # è‡ªåŠ¨ä»»åŠ¡å®Œæˆ

    # äº‹ä»¶è¿›åº¦æƒé‡é…ç½®ï¼ˆå·²ä¼˜åŒ–é™ä½ï¼‰
    combat_victory_weight: float = 3.0         # æˆ˜æ–—èƒœåˆ©è¿›åº¦æƒé‡ï¼ˆåŸ5.0ï¼‰
    exploration_weight: float = 1.5            # æ¢ç´¢è¿›åº¦æƒé‡ï¼ˆåŸ2.0ï¼‰
    story_event_weight: float = 8.0            # å‰§æƒ…äº‹ä»¶è¿›åº¦æƒé‡ï¼ˆåŸ10.0ï¼‰
    treasure_found_weight: float = 2.0         # å‘ç°å®è—è¿›åº¦æƒé‡ï¼ˆåŸ3.0ï¼‰

    # è¿›åº¦è§¦å‘é˜ˆå€¼
    major_progress_threshold: float = 25.0     # é‡å¤§è¿›åº¦é˜ˆå€¼ï¼ˆè§¦å‘ç‰¹æ®Šäº‹ä»¶ï¼‰
    completion_threshold: float = 100.0        # å®Œæˆé˜ˆå€¼
    near_completion_threshold: float = 80.0    # æ¥è¿‘å®Œæˆé˜ˆå€¼


@dataclass
class WebConfig:
    """WebæœåŠ¡é…ç½®ç±»"""
    host: str = "127.0.0.1"
    port: int = 8001
    reload: bool = False

    # é™æ€æ–‡ä»¶é…ç½®
    static_dir: str = "static"
    templates_dir: str = "templates"

    # å®‰å…¨é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡åŠ è½½ï¼Œè§ _load_from_envï¼‰
    secret_key: str = "labyrinthia-ai-secret-key-change-in-production"
    cors_origins: list = field(default_factory=lambda: ["*"])

    # ä¼šè¯é…ç½®
    session_timeout: int = 3600  # ç§’


@dataclass
class DataConfig:
    """æ•°æ®å­˜å‚¨é…ç½®ç±»"""
    # æ•°æ®ç›®å½•
    data_dir: str = "data"
    saves_dir: str = "saves"
    cache_dir: str = "cache"
    logs_dir: str = "logs"

    # æ•°æ®æ–‡ä»¶æ ¼å¼
    data_format: str = "json"  # json, yaml, pickle
    compression: bool = False

    # å¤‡ä»½è®¾ç½®
    enable_backup: bool = True
    backup_interval: int = 86400  # ç§’ï¼ˆ24å°æ—¶ï¼‰
    max_backups: int = 7


class Config:
    """ä¸»é…ç½®ç±»"""

    def __init__(self):
        self.llm = LLMConfig()
        self.game = GameConfig()
        self.web = WebConfig()
        self.data = DataConfig()
        self.debug = DebugConfig()

        # æ£€æŸ¥ .env æ–‡ä»¶
        self._check_env_file()

        # ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
        self._load_from_env()

        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self._create_directories()

        # éªŒè¯å…³é”®é…ç½®
        self._validate_critical_config()

    def _load_from_env(self):
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        # ------------------- Load LLM Configuration -------------------
        # 1. ä»ç¯å¢ƒå˜é‡ç¡®å®šLLM Provider
        if provider_env := os.getenv("LLM_PROVIDER"):
            try:
                self.llm.provider = LLMProvider(provider_env.lower())
            except ValueError:
                pass  # ä¿ç•™ä»£ç ä¸­è®¾ç½®çš„é»˜è®¤å€¼

        # 2. åŠ è½½å¯¹åº”Providerçš„ç¯å¢ƒå˜é‡ (å¦‚æœå­˜åœ¨)
        # Gemini
        if gemini_api_key := os.getenv("GEMINI_API_KEY"):
            self.llm.gemini.api_key = gemini_api_key
        if gemini_model := os.getenv("GEMINI_MODEL_NAME"):
            self.llm.gemini.model_name = gemini_model
        # OpenRouter
        if openrouter_api_key := os.getenv("OPENROUTER_API_KEY"):
            self.llm.openrouter.api_key = openrouter_api_key
        if openrouter_model := os.getenv("OPENROUTER_MODEL_NAME"):
            self.llm.openrouter.model_name = openrouter_model
        # OpenAI
        if openai_api_key := os.getenv("OPENAI_API_KEY"):
            self.llm.openai.api_key = openai_api_key
        if openai_model := os.getenv("OPENAI_MODEL_NAME"):
            self.llm.openai.model_name = openai_model
        if openai_base_url := os.getenv("OPENAI_BASE_URL"):
            self.llm.openai.base_url = openai_base_url
        # LMStudio
        if lmstudio_base_url := os.getenv("LMSTUDIO_BASE_URL"):
            self.llm.lmstudio.base_url = lmstudio_base_url
        if lmstudio_model := os.getenv("LMSTUDIO_MODEL_NAME"):
            self.llm.lmstudio.model_name = lmstudio_model

        # 3. æ ¹æ®æœ€ç»ˆç¡®å®šçš„providerï¼ŒåŠ¨æ€å¡«å……Active LLM Configuration
        provider_config_map = {
            LLMProvider.GEMINI: self.llm.gemini,
            LLMProvider.OPENROUTER: self.llm.openrouter,
            LLMProvider.OPENAI: self.llm.openai,
            LLMProvider.LMSTUDIO: self.llm.lmstudio,
        }

        active_provider_config = provider_config_map.get(self.llm.provider)

        if active_provider_config:
            self.llm.api_key = getattr(active_provider_config, 'api_key', '')
            self.llm.model_name = getattr(active_provider_config, 'model_name', '')

            # å¡«å……ç‰¹å®šäºæä¾›å•†çš„URL
            self.llm.gemini_endpoint = self.llm.gemini.endpoint
            self.llm.gemini_api_version = self.llm.gemini.api_version
            self.llm.openrouter_base_url = self.llm.openrouter.base_url
            self.llm.openai_base_url = self.llm.openai.base_url
            self.llm.lmstudio_base_url = self.llm.lmstudio.base_url

        # ------------------- Load Proxy Configuration -------------------
        if proxy_url := os.getenv("PROXY_URL"):
            self.llm.proxy_url = proxy_url

        if use_proxy := os.getenv("USE_PROXY"):
            self.llm.use_proxy = use_proxy.lower() in ("true", "1", "yes")

        if proxy_username := os.getenv("PROXY_USERNAME"):
            self.llm.proxy_username = proxy_username

        if proxy_password := os.getenv("PROXY_PASSWORD"):
            self.llm.proxy_password = proxy_password

        # Webé…ç½®
        if host := os.getenv("WEB_HOST"):
            self.web.host = host

        if port := os.getenv("WEB_PORT"):
            try:
                self.web.port = int(port)
            except ValueError:
                pass

        # æ¸¸æˆé…ç½®
        if debug := os.getenv("DEBUG"):
            self.game.debug_mode = debug.lower() in ("true", "1", "yes")

        if show_llm_debug := os.getenv("SHOW_LLM_DEBUG"):
            self.game.show_llm_debug = show_llm_debug.lower() in ("true", "1", "yes")

        # è°ƒè¯•é…ç½®
        if debug_enabled := os.getenv("DEBUG_ENABLED"):
            self.debug.enabled = debug_enabled.lower() in ("true", "1", "yes")

        if show_encoding_debug := os.getenv("SHOW_ENCODING_DEBUG"):
            self.debug.show_encoding_debug = show_encoding_debug.lower() in ("true", "1", "yes")

        if show_performance_metrics := os.getenv("SHOW_PERFORMANCE_METRICS"):
            self.debug.show_performance_metrics = show_performance_metrics.lower() in ("true", "1", "yes")

        # ç¼–ç é…ç½®
        if use_encoding := os.getenv("USE_ENCODING_CONVERSION"):
            self.llm.use_encoding_conversion = use_encoding.lower() in ("true", "1", "yes")

        if encoding_method := os.getenv("ENCODING_METHOD"):
            if encoding_method in ["utf8_strict", "json_escape"]:
                self.llm.encoding_method = encoding_method

        # ------------------- Load Security Configuration -------------------
        if secret_key := os.getenv("SECRET_KEY"):
            self.web.secret_key = secret_key

        # ------------------- Load LLM Advanced Configuration -------------------
        if llm_timeout := os.getenv("LLM_TIMEOUT"):
            try:
                self.llm.timeout = int(llm_timeout)
            except ValueError:
                pass

        if llm_max_output := os.getenv("LLM_MAX_OUTPUT_TOKENS"):
            if llm_max_output.strip().lower() in ("none", ""):
                self.llm.max_output_tokens = None
            else:
                try:
                    self.llm.max_output_tokens = int(llm_max_output)
                except ValueError:
                    pass

        if llm_use_gen_params := os.getenv("LLM_USE_GENERATION_PARAMS"):
            self.llm.use_generation_params = llm_use_gen_params.lower() in ("true", "1", "yes")

        if llm_temperature := os.getenv("LLM_TEMPERATURE"):
            try:
                self.llm.temperature = float(llm_temperature)
            except ValueError:
                pass

        if llm_top_p := os.getenv("LLM_TOP_P"):
            try:
                self.llm.top_p = float(llm_top_p)
            except ValueError:
                pass

        # ------------------- Load LLM Context Management Configuration -------------------
        if max_history_tokens := os.getenv("LLM_MAX_HISTORY_TOKENS"):
            try:
                self.llm.max_history_tokens = int(max_history_tokens)
            except ValueError:
                pass

        if min_context_entries := os.getenv("LLM_MIN_CONTEXT_ENTRIES"):
            try:
                self.llm.min_context_entries = int(min_context_entries)
            except ValueError:
                pass

        if cleanup_threshold := os.getenv("LLM_CONTEXT_CLEANUP_THRESHOLD"):
            try:
                self.llm.context_cleanup_threshold = float(cleanup_threshold)
            except ValueError:
                pass

        #
        #
        if save_ctx_entries := os.getenv("LLM_SAVE_CONTEXT_ENTRIES"):
            try:
                self.llm.save_context_entries = int(save_ctx_entries)
            except ValueError:
                pass


        # LLM ä¸Šä¸‹æ–‡è®°å½•å¼€å…³ï¼ˆç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
        if record_combat := os.getenv("LLM_RECORD_COMBAT_TO_CONTEXT"):
            self.llm.record_combat_to_context = record_combat.lower() in ("true", "1", "yes")
        if record_trap := os.getenv("LLM_RECORD_TRAP_TO_CONTEXT"):
            self.llm.record_trap_to_context = record_trap.lower() in ("true", "1", "yes")

        # ------------------- Load Game Session Configuration -------------------
        if auto_save_interval := os.getenv("AUTO_SAVE_INTERVAL"):
            try:
                self.game.auto_save_interval = int(auto_save_interval)
            except ValueError:
                pass

        if game_session_timeout := os.getenv("GAME_SESSION_TIMEOUT"):
            try:
                self.game.game_session_timeout = int(game_session_timeout)
            except ValueError:
                pass

        if max_active_games := os.getenv("MAX_ACTIVE_GAMES_PER_USER"):
            try:
                self.game.max_active_games_per_user = int(max_active_games)
            except ValueError:
                pass

        # ------------------- Load Performance Configuration -------------------
        if max_concurrent_llm := os.getenv("MAX_CONCURRENT_LLM_REQUESTS"):
            try:
                self.game.max_concurrent_llm_requests = int(max_concurrent_llm)
            except ValueError:
                pass

        if retry_count := os.getenv("REQUEST_RETRY_COUNT"):
            try:
                self.game.request_retry_count = int(retry_count)
            except ValueError:
                pass

        if retry_delay := os.getenv("REQUEST_RETRY_DELAY"):
            try:
                self.game.request_retry_delay = float(retry_delay)
            except ValueError:
                pass

    def _check_env_file(self):
        """æ£€æŸ¥ .env æ–‡ä»¶å¹¶æç¤ºç”¨æˆ·"""
        env_path = Path(".env")
        env_example_path = Path(".env.example")

        if not _env_loaded:
            if not env_path.exists():
                logger.warning("âš ï¸  .env æ–‡ä»¶ä¸å­˜åœ¨")
                if env_example_path.exists():
                    logger.info("ğŸ’¡ æç¤º: å¤åˆ¶ .env.example ä¸º .env å¹¶é…ç½®æ‚¨çš„ API å¯†é’¥")
                    logger.info("   å‘½ä»¤: copy .env.example .env  (Windows)")
                    logger.info("   å‘½ä»¤: cp .env.example .env    (Linux/Mac)")
                else:
                    logger.info("ğŸ’¡ æç¤º: åˆ›å»º .env æ–‡ä»¶å¹¶é…ç½®æ‚¨çš„ API å¯†é’¥")
                logger.info("   æˆ–è€…ç›´æ¥åœ¨ config.py ä¸­ä¿®æ”¹é»˜è®¤é…ç½®")
            else:
                logger.debug("âœ“ .env æ–‡ä»¶å·²åŠ è½½")

    def _validate_critical_config(self):
        """éªŒè¯å…³é”®é…ç½®"""
        # æ£€æŸ¥ API Key
        if not self.llm.api_key:
            logger.warning("âš ï¸  LLM API Key æœªè®¾ç½®")
            logger.info(f"ğŸ’¡ å½“å‰ä½¿ç”¨çš„ Provider: {self.llm.provider.value}")

            # æ ¹æ®ä¸åŒçš„ Provider ç»™å‡ºå…·ä½“æç¤º
            provider_key_map = {
                LLMProvider.GEMINI: "GEMINI_API_KEY",
                LLMProvider.OPENROUTER: "OPENROUTER_API_KEY",
                LLMProvider.OPENAI: "OPENAI_API_KEY",
                LLMProvider.LMSTUDIO: "LMSTUDIO_BASE_URL"
            }

            key_name = provider_key_map.get(self.llm.provider, "API_KEY")
            logger.info(f"   è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: {key_name}=your_key_here")
        else:
            logger.debug(f"âœ“ API Key å·²é…ç½® (Provider: {self.llm.provider.value})")

    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [
            self.data.data_dir,
            self.data.saves_dir,
            self.data.cache_dir,
            self.data.logs_dir,
            self.web.static_dir,
            self.web.templates_dir,
        ]

        for directory in directories:
            try:
                os.makedirs(directory, exist_ok=True)
                logger.debug(f"âœ“ ç›®å½•å·²å°±ç»ª: {directory}")
            except PermissionError:
                logger.error(f"âŒ æƒé™ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºç›®å½•: {directory}")
                logger.error("   è¯·æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿæƒé™æˆ–ä½¿ç”¨ç®¡ç†å‘˜æƒé™è¿è¡Œ")
                raise
            except OSError as e:
                logger.error(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥: {directory}")
                logger.error(f"   é”™è¯¯ä¿¡æ¯: {e}")
                raise

    def update_config(self, section: str, **kwargs):
        """æ›´æ–°é…ç½®"""
        if hasattr(self, section):
            config_section = getattr(self, section)
            for key, value in kwargs.items():
                if hasattr(config_section, key):
                    setattr(config_section, key, value)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "llm": self.llm.__dict__,
            "game": self.game.__dict__,
            "web": self.web.__dict__,
            "data": self.data.__dict__,
            "debug": self.debug.__dict__,
        }


# å…¨å±€é…ç½®å®ä¾‹
config = Config()

# å¯¼å‡ºå¸¸ç”¨é…ç½®
__all__ = [
    "Config",
    "LLMConfig",
    "GameConfig",
    "WebConfig",
    "DataConfig",
    "DebugConfig",
    "LLMProvider",
    "config"
]
