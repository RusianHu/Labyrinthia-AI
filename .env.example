# Labyrinthia AI - 环境变量配置示例
# 复制此文件为 .env 并填入你的实际配置

# ==================== LLM Provider Configuration ====================
# 可选项: gemini | openai | openrouter | lmstudio
LLM_PROVIDER=openai

# ==================== Gemini Configuration ====================
GEMINI_API_KEY=your_gemini_api_key_here
# 可选项: gemini-2.0-flash | gemini-2.0-flash-exp | gemini-1.5-pro | gemini-1.5-flash
GEMINI_MODEL_NAME=gemini-2.0-flash

# ==================== OpenRouter Configuration ====================
OPENROUTER_API_KEY=your_openrouter_api_key_here
# 可选项: google/gemini-2.0-flash-001 | anthropic/claude-3.5-sonnet | openai/gpt-4o 等
OPENROUTER_MODEL_NAME=google/gemini-2.0-flash-001

# ==================== OpenAI Configuration ====================
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL_NAME=gemini-2.0-flash
OPENAI_BASE_URL=https://ai.yanshanlaosiji.top/v1

# ==================== LMStudio Configuration ====================
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_MODEL_NAME=local-model

# ==================== Proxy Configuration ====================
# 可选项: true | false
USE_PROXY=false
PROXY_URL=http://127.0.0.1:10808
PROXY_USERNAME=
PROXY_PASSWORD=

# ==================== Web Server Configuration ====================
WEB_HOST=127.0.0.1
WEB_PORT=8001

# ==================== Debug Configuration ====================
# 调试配置 - 控制各种调试功能的开关
# 可选项: true | false

# 【主调试开关】启用调试模式
# 作用：控制 encoding_utils.py 中编码转换的调试信息输出
#       在 gemini_api.py 和 openrouter_client.py 中启用请求大小影响计算
#       影响日志输出级别和详细程度
# 建议：开发环境 true，生产环境 false
DEBUG=true

# 【LLM调试信息显示】显示LLM请求和响应的详细信息 ⭐ 最常用
# 作用：在前端显示 FAB 风格的调试按钮（DebugManager.js）
#       启用 /api/debug/llm-info 端点获取最后的LLM请求和响应
#       在 event_choice_system.py 中记录选择处理的详细日志
#       控制快速测试页面 quick_test.html 的显示
# 使用场景：调试LLM交互问题、优化提示词、排查生成内容异常时启用
# 建议：开发环境 true，生产环境 false
SHOW_LLM_DEBUG=true

# 【调试功能总开关】与 DEBUG 类似，控制调试功能的启用
# 作用：控制 encoding_utils.py 中编码转换的调试信息输出
#       在 gemini_api.py 和 openrouter_client.py 中启用请求大小影响计算
# 注意：通常与 DEBUG 保持一致
# 建议：开发环境 true，生产环境 false
DEBUG_ENABLED=true

# 【编码调试信息】显示字符编码转换的调试信息
# 作用：在 encoding_utils.py 中记录编码操作的成功/失败信息
#       显示编码方法的应用情况（utf8_strict 或 json_escape）
#       记录编码操作的耗时和重试次数
# 使用场景：解决Ubuntu服务器字符编码问题、排查中文显示异常时启用
# 建议：仅在遇到编码问题时启用，否则关闭以减少日志噪音
SHOW_ENCODING_DEBUG=true

# 【性能指标显示】显示性能相关的统计指标
# 作用：在 encoding_utils.py 中输出编码性能摘要
#       显示编码操作的统计信息（成功率、平均耗时、大小增长等）
#       帮助识别性能瓶颈
# 使用场景：性能优化、识别慢操作、评估编码转换开销时启用
# 建议：性能调优时启用，日常开发可关闭
SHOW_PERFORMANCE_METRICS=true

# ==================== Encoding Configuration ====================
# 编码配置 - 用于解决不同操作系统的字符编码问题
# 可选项: true | false

# 【编码转换开关】是否启用编码转换功能
# 作用：解决Ubuntu服务器上的中文字符编码问题
#       确保LLM请求中的中文字符正确编码
# 使用场景：Ubuntu/Linux服务器环境建议启用
#          Windows环境通常不需要启用
# 建议：Windows环境 false，Ubuntu/Linux环境 true
USE_ENCODING_CONVERSION=false

# 【编码方法】选择编码转换的方法
# 可选项: utf8_strict | json_escape
# utf8_strict: 严格的UTF-8编码，确保所有字符串都是有效的UTF-8
# json_escape: JSON转义方法，将特殊字符转义为\uXXXX格式
# 建议：优先使用 utf8_strict，如果遇到问题再尝试 json_escape
ENCODING_METHOD=utf8_strict

# ==================== Security Configuration ====================
# Web应用安全密钥 - 用于会话加密等安全功能
# ⚠️ 生产环境必须修改为强随机字符串！
# 建议使用: python -c "import secrets; print(secrets.token_urlsafe(32))" 生成
SECRET_KEY=your-secret-key-change-in-production

# ==================== LLM Advanced Configuration ====================
# LLM超时时间（秒）
# 建议：根据网络状况和模型响应速度调整，默认120秒
LLM_TIMEOUT=120

# LLM最大输出token数
# 留空或设置为 "None" 表示不限制（推荐，避免思考过程被截断）
# 设置具体数值时注意：某些模型（如gemini-2.5-flash）有内部思考过程会消耗大量token
LLM_MAX_OUTPUT_TOKENS=

# LLM生成参数
# 是否使用temperature和top_p参数（false时使用LLM默认值）
LLM_USE_GENERATION_PARAMS=false
# Temperature控制随机性 (0.0-2.0)，值越高输出越随机
LLM_TEMPERATURE=0.8
# Top-p控制多样性 (0.0-1.0)，核采样参数
LLM_TOP_P=0.9

# ==================== LLM Context Management ====================
# LLM上下文管理配置

# 最大上下文token数量
# 建议：10240（约4000-5000个中文字符）
# 说明：控制传递给LLM的历史上下文长度，避免超出模型限制
LLM_MAX_HISTORY_TOKENS=10240

# 保留的最小上下文条目数
# 建议：5-10条
# 说明：即使超过token限制，也会保留最近的N条记录
LLM_MIN_CONTEXT_ENTRIES=5

# 上下文清理触发阈值（百分比，0.0-1.0）
# 建议：0.8（80%）
# 说明：当上下文使用量达到此阈值时触发自动清理
LLM_CONTEXT_CLEANUP_THRESHOLD=0.8

# ==================== Game Session Configuration ====================
# 自动保存间隔（秒）
# 建议：300秒（5分钟）平衡性能和数据安全
AUTO_SAVE_INTERVAL=300

# 游戏会话超时时间（秒），无活动后自动关闭
# 建议：3600秒（1小时）
GAME_SESSION_TIMEOUT=3600

# 每个用户最多同时活跃的游戏数量
# 建议：5个，防止资源滥用
MAX_ACTIVE_GAMES_PER_USER=5

# ==================== Performance Configuration ====================
# 最大并发LLM请求数
# 建议：3个，避免API限流和资源耗尽
MAX_CONCURRENT_LLM_REQUESTS=3

# 请求重试次数
# 建议：3次，平衡可靠性和响应时间
REQUEST_RETRY_COUNT=3

# 请求重试延迟（秒）
# 建议：1.0秒，使用指数退避策略
REQUEST_RETRY_DELAY=1.0

